



# Ubuntu 터미널에서 Text Manipulation으로 Nginx 로그 분석하기  
— `awk | sort | uniq | sort | head` 파이프라인 완전 이해

서버/DevOps 작업을 하다 보면 결국 매일 보는 건 **텍스트**입니다.

- `/var/log/nginx/access.log` 같은 **로그 파일**
- `systemctl status`, `ss -lntp`, `ps aux` 같은 **명령어 출력**
- `nginx.conf`, `quizai.service`, `.env` 같은 **설정 파일**

이 텍스트들을 **터미널에서 빠르게 자르고, 골라서, 집계하는 기술**이 바로  
Ubuntu에서 말하는 **Text Manipulation**입니다.

이번 글에서는 실제로 내가 쓴 예시인,

> **“Nginx access 로그에서 많이 요청한 IP TOP 10 뽑기”**

를 기준으로, Text Manipulation 파이프라인을 정리해 봅니다.

---

## 1. 목표: 많이 요청한 IP TOP 10 뽑기

Nginx access 로그(`/var/log/nginx/access.log`)를 보면 한 줄에 이런 정보들이 들어 있습니다.

- 요청한 IP
- 요청 시간
- HTTP 메서드/URL
- 응답 코드(200, 404, 500 등)
- User-Agent …

여기서 **“어떤 IP가 얼마나 자주 요청을 보내는지”** 알고 싶다고 해볼게요.  
이를 위해 사용한 명령어는 다음과 같습니다.

```bash
awk '{print $1}' /var/log/nginx/access.log |
  sort |
  uniq -c |
  sort -nr |
  head
````

이 한 줄이 하는 일을 단계별로 뜯어서 보면 Text Manipulation의 핵심이 다 들어 있습니다.

---

## 2. 파이프라인 한 줄씩 뜯어보기

### 2-1. `awk '{print $1}'` – 첫 번째 컬럼(IP)만 뽑기

```bash
awk '{print $1}' /var/log/nginx/access.log
```

* `awk`는 **텍스트를 “필드(컬럼)” 단위로 다루는 도구**
* 기본적으로 공백을 기준으로 나누고,
  첫 번째 필드는 `$1`, 두 번째는 `$2`, 전체 줄은 `$0`

여기서는 `'{print $1}'` 이므로,

> **각 줄에서 1번째 필드(IP 주소)만 출력**

하게 됩니다.

---

### 2-2. `sort` – 같은 IP끼리 붙도록 정렬

```bash
... | sort
```

* IP가 마구 섞여 있으면 개수 세기 어렵기 때문에
  **같은 값끼리 모이도록 정렬**해 줍니다.
* 이 단계까지 하면, 출력은 “정렬된 IP 리스트”가 됩니다.

---

### 2-3. `uniq -c` – 중복 개수 세기

```bash
... | uniq -c
```

* `uniq`는 **연속된 중복 줄을 하나로 합치고**,
  `-c` 옵션을 주면 **각 값이 몇 번 나왔는지 개수까지 앞에 붙여줌**.

예시 형태:

```text
     35 192.159.99.95
      5 3.131.215.38
      4 65.49.20.66
      ...
```

* 앞 숫자: 해당 IP가 나온 횟수 (요청 수)
* 뒤 IP: 실제 클라이언트 IP

---

### 2-4. `sort -nr` – 많이 나온 IP 순으로 정렬

```bash
... | sort -nr
```

* `-n`: 숫자 기준으로 정렬
* `-r`: 내림차순(큰 값 → 작은 값)

즉, **요청 횟수가 많은 IP 순서대로** 정렬합니다.

---

### 2-5. `head` – 상위 10개만 보기

```bash
... | head
```

* 기본값으로 **앞 10줄만 출력**
* 결과적으로,

  > **가장 많이 요청을 보낸 IP TOP 10**
  > 을 한 번에 확인할 수 있습니다.

---

## 3. 최종 결과 해석 예시

실제 명령 실행 결과는 다음과 같이 나왔습니다.

```text
     35 192.159.99.95
      5 3.131.215.38
      4 65.49.20.66
      3 45.82.78.103
      3 162.158.91.14
      2 91.92.242.214
      2 87.121.84.52
      2 31.59.33.197
      2 31.58.24.71
      2 198.235.24.169
```

각 줄의 의미는:

* `35 192.159.99.95`
  → `192.159.99.95` 라는 IP가 access.log에서 **35번** 등장
  → 이 IP가 서버에 **35번 요청을 보냄**

* `5 3.131.215.38`
  → 이 IP는 5번 요청을 보냄

이렇게 **앞 숫자 = 요청 횟수, 뒤 = IP 주소**로 해석하면 됩니다.

---

## 4. 이게 왜 DevOps에서 중요한가?

이 짧은 파이프라인 하나에 DevOps에서 자주 쓰는 Text Manipulation 개념이 다 들어 있습니다.

* **필드 추출**: `awk '{print $1}'`
* **정렬**: `sort`
* **중복 카운트**: `uniq -c`
* **숫자 기준 내림차순 정렬**: `sort -nr`
* **상위 N개만 보기**: `head`
* **파이프 연결**: `|` 로 여러 명령을 하나의 “데이터 흐름”으로 묶기

이 패턴을 응용하면:

* **500 에러 많이 발생한 IP TOP 10**
* **특정 URL을 제일 많이 호출한 IP**
* **특정 시간대에 가장 많은 요청이 들어온 시간대 찾기**

같은 분석도 전부 **터미널에서 몇 줄로 처리**할 수 있습니다.

---

## 5. 정리

* DevOps에서 말하는 **Text Manipulation**은
  단순히 문자열 자르기가 아니라,

  > 로그/출력/설정 파일 등 텍스트를
  > **터미널에서 조합 명령으로 빠르게 필터·집계·변형하는 기술**

* 이번에 사용한 명령어:

```bash
awk '{print $1}' /var/log/nginx/access.log |
  sort |
  uniq -c |
  sort -nr |
  head
```

* 이 한 줄로,

  > **“Nginx access 로그에서 가장 많이 서버에 요청을 보낸 IP 10개”**
  > 를 바로 뽑아낼 수 있다.

이 글에서 정리한 패턴만 익숙해져도,
Ubuntu 서버에서 로그를 다루는 감각이 훨씬 좋아질 거다.

```markdown
(필요하면 다음 글에서 `grep`이랑 `sed`로
에러 로그 필터링 + 포트/경로 일괄 변경도 정리 예정)
```

```
```
